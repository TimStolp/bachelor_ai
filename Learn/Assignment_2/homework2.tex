\documentclass[11pt,a4paper]{article}

\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{dsfont}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\title{Leren --- Homework 2\\[3mm]\small{Chapter 4 \& 5, Alpaydin}}
\date{Deadline: 23:59 November 13th, 2018}


\begin{document}

\maketitle

\newcommand{\LL}{\mathcal{L}(\bmu, \bSigma|\mathcal{\bX})}
\newcommand{\LLp}{\mathcal{L}(\textbf{p}|\mathcal{\bX})}

\newcommand{\dermuLL}{\frac{\partial\mathcal{L}(\bmu, \bSigma|\mathcal{\bX})}{\partial \bmu}}
\newcommand{\dersigmaLL}{\frac{\partial \mathcal{L}(\mu, \sigma|\mathcal{X})}{\partial \sigma^2}}
\newcommand{\derpLLp}{\frac{\partial\LLp}{\partial p_k}}

This is the third week's assignment for Leren.  This assignment covers
chapter 4 \& 5 of Alpaydin. Please take note of the following:

\begin{itemize}
  \item You are expected to hand in your solutions in \LaTeX;
  \item This problem set is an individual assignment;
  \item The deadline for this first assignment is Tuesday, November 13 at 23:59.
\end{itemize}


\section{Background: Linearity of the expectation}
Use the definition of the expectation to answer the following
questions:
\begin{enumerate}[(a)]
	\item For a random variable $X$,
		\begin{align}
			\mathds{E}[a X + b] &= a \mathds{E}[X] + b
		\end{align}
		Show that this is the case.
	\item For random variables $X$ and $Y$,
		\begin{align}
			\mathds{E}[X + Y] = \mathds{E}[X] + \mathds{E}[Y]
		\end{align}
		Show that this is the case.
\end{enumerate}

\section{Chapter 4: Parametric Methods}

\subsection{Bias/Variance}
\begin{enumerate}[(a)]
		\item  We have some model $g(x)$ that has a very high bias. What does that mean in terms of the training and validation errors?
		\item We also consider some other model, $h(x)$ that has a very high variance. What does that mean in terms of the training and validation errors?
		\item Suppose we use a massive dataset for training both models, which one is likely to give a better performance? Will the model that performs worse overfit or underfit?
\end{enumerate}

\subsection{MAP Estimation of Gaussian (normal) Density}

\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bm}{\boldsymbol{m}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bX}{\boldsymbol{X}}

In this exercise, we will lead you step-by-step to derive the Maximum A Posteriori (MAP) estimate of the mean of a Gaussian density. Recall that the MAP is given by $\theta_{MAP} = \underset{\theta}{\text{ arg max }} p(\theta|X)$ (where in this case, the parameter $\theta$ is $\mu$).
Assume we are trying to fit a Gaussian to a dataset $X$ with $N$ data points $x_i$.
\begin{align}
	P(X|\mu, \sigma_0) =
	\frac{1}{(2\pi)^{N/2} \sigma_0^{N}} \exp
	\left \{ -\frac{\sum^N_{i=1} (\text{x}_i -
	\mu)^2}{2\sigma_0^2}  \right \} \label{likelihood}
\end{align}
where we have placed a prior on $\mu$:
\begin{align}
	P(\mu|m, \sigma_1) =
	\frac{1}{\sqrt{2\pi} \sigma_1} \exp
	\left \{ -\frac{ (\mu -
	m)^2}{2\sigma_1^2}  \right \} \label{prior}
\end{align}
\begin{enumerate}[(a)]
	\item Write down the posterior for this model, assuming that $\sigma_0$, $m$ and $\sigma_1$ are given. 
	\item Write down the log of the posterior and fill in Eq.~\eqref{likelihood} and Eq.~\eqref{prior}. Since $P(x)$ does not depend on $\mu$, you can follow Eq.~4.39 from the book and write it as a constant $c$.
	\item Take the derivative of the log posterior wrt $\mu$.
	\item To obtain the MAP, we have to find the point where the log posterior is maximized. To find this point, set the derivative to 0 and solve for $\mu$.
\end{enumerate}


\subsection{Estimators}
Show that the estimator for the mean of a Gaussian density, $m = \frac{\sum_t x^t}{N}$ is a consistent estimator for the true mean $\mu$, i.e. that \text{Var}($m$)$\rightarrow 0$ as $N \rightarrow \infty$, assuming that the data is i.i.d. \\ Hint: $\text{Var}(\sum_i x_i) = \sum_i \text{Var}(x_i)$ for i.i.d.

\section{Chapter 5: Multivariate Methods}
\subsection{Maximum Likelihood of a Multivariate Gaussian Density}
Given a dataset $\bX = \{\bx_i \}_{i=1}^N$ with $N$ $d$-dimensional (iid) data points.
We assume that $\bx_i$ are drawn from a Multivariate Gaussian distribution such that the likelihood function can be expressed as  $l(\bmu, \bSigma | \bX) \equiv P(\bX|\bmu, \bSigma)$:
\begin{align}
	P(\bX|\bmu, \bSigma) =
	\frac{1}{(2\bpi)^{N\cdot D/2} | \bSigma |^{N/2}} \exp
	\left \{ -\frac{1}{2} \sum^N_{i=1} (\bx_i -
	\bmu)^T\bSigma^{-1} (\bx_i -
	\bmu) \right \} \label{eq:multivariate_likelihood}
\end{align}
Derive the maximum likelihood estimator for the mean of the Gaussian. To do so, take the following steps:
		\begin{enumerate}[(1)]
			\item Write down the expression for the log-likelihood $\LL \equiv \text{log }l(\bmu, \bSigma|\bX)$
			\item Take the derivative of the log-likelihood wrt $\bmu$ (Hint: $\frac{\partial (\textbf{a}^T \bSigma \textbf{a})}{\partial \textbf{a}} = 2 \textbf{a}^T \bSigma$)
			\item Set the derivative to $0$
			\item Solve for $\bmu$
		\end{enumerate}

\end{document}
