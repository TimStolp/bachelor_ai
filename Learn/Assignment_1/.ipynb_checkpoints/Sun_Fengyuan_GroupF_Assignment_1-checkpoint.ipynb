{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Fengyuan Sun\n",
    "\n",
    "### Student ID: 11697318\n",
    "\n",
    "### Group: F\n",
    "\n",
    "Please fill in you name, student ID and group above, and also edit the filename according to the specified format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression and Cross Validation\n",
    "\n",
    "For the first assignment we will do something that might seem familiar from *Probability Theory for Machine Learning*; try to fit a polynomial function to a provided dataset. Fitting a function is a quintessential example of *supervised learning*, specifically *regression*, making it a great place to start using machines to learn about *machine learning*. There are several concepts here that are applicable to lots of *supervised learning* algorithms, so it will be good to cover them in a familiar context first.\n",
    "\n",
    "The notion of a *cost function* will be introduced here, which describes how well a given model fits the provided data. This function can then be minimized in several different ways, depending on complexity of the model and associated cost function, e.g. using *gradient descent* to iteratively approach the minimum or computing the minimum directly using an analytic method, both of which you may have seen some version of before.\n",
    "\n",
    "We will start with the most basic model (linear) and compute the parameters that minimize the cost function directly, based on the derivate. It is important that you try and comprehend what you are doing in this most basic version (instead of just blindly trying to implement functions until they seem to work), as it will help you understand the more complex models that use the same principles later on. This means actually **watching the linked videos** and computing the partial derivates yourself to verify you understand all of the steps. \n",
    "\n",
    "The other common concept introduced is model selection using *cross validation*. In this assignment it will be used to determine the degree of the polynomial we are fitting. Both cross validation for model selection and minizing a the cost function to achieve the best possible fit, are used in many other supervised models, like for example *neural networks*.\n",
    "\n",
    "## Material\n",
    "\n",
    "The material for this assignment is based on sections **2.6 - 2.8** and **4.6 - 4.8** of the book *[Introduction to Machine Learning](https://www.cmpe.boun.edu.tr/~ethem/i2ml3e/)* by Ethem Alpaydin. In addition, there will be links to videos from Andrew Ng's *[Machine Learning course on Coursera](https://www.coursera.org/learn/machine-learning)* to provide some extra explanations and help create intuitions.\n",
    "\n",
    "Generally speaking, using built-in functions will be fine for this course, but for this assignment you **may not** use any of the polynomial functions listed [here](https://docs.scipy.org/doc/numpy/reference/routines.polynomials.poly1d.html) or other built-in polynomial solution methods. You can of course use them to check your own implementations work correctly.\n",
    "\n",
    "In total there are *27* points available in this exercise. Below are some imports to get your started. You do not need to add any code for this cell to work, just make sure you run the cell to actually import the libraries in your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data [1 pt]\n",
    "\n",
    "Write a function to read the data stored in the file *points.csv* and convert it to a *Numpy* array. Each line in the file is a data point consisting of an **x**-value and **r**-value, separated by a comma. You could use Numpy's [loadtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html), or any other method of your choice to read csv-files and convert that to the correct type.\n",
    "\n",
    "Test your function and print the resulting array to make sure you know what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the points [2 pt]\n",
    "\n",
    "Write a function to separate your data into an X vector and a R vector using [slicing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html).\n",
    "\n",
    "Using both vectors, create a graph containing the plotted points you just read from the file. For this you can use the *matplotlib* functions [plot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) and [show](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.show). A plot of data should be visble below your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5d16ac780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_X_R(data):\n",
    "    X = data[:,0]\n",
    "    R = data[:,1]\n",
    "    return X, R\n",
    "\n",
    "data = load_file(\"points.csv\")\n",
    "X, R = split_X_R(data)\n",
    "\n",
    "\n",
    "plt.plot(X, R, 'go')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the linear model [1 pt]\n",
    "\n",
    "Now we are going to try to find the function which best relates these points. We will start by fitting a simple linear function of the form\n",
    "\n",
    "(2.15) $$g(x) = w_1x + w_0$$\n",
    "\n",
    "*For more detailed description of linear regression, watch Andrew's videos on the topic. The notation is slightly different, $y$ instead of $r$ for the output, and $\\theta$ instead of $w$ for the model parameters, but the actual model is identical.*\n",
    "* [Supervised Learning](https://www.youtube.com/watch?v=ls7Ke48jCt8&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=3)\n",
    "* [Linear Model](https://www.youtube.com/watch?v=PBZUjnGuXjA&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=5)\n",
    "\n",
    "Now write a function that computes the predicted output value $g(x)$ given a value of $x$ and the parameters $w_0$ and $w_1$. This should be very straightforward, but make sure you understand what part this plays in our supervised learning problem before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(w0, w1, x):\n",
    "    return w1*x + w0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cost function [2 pt]\n",
    "\n",
    "The cost function is defined as the sum of the squared errors of each prediction\n",
    "\n",
    "(2.16) $$E(w_1, w_0|X) = \\frac{1}{N}\\sum^N_{t=1} [r^t - (w_1x^t + w_0)]^2$$\n",
    "\n",
    "*These videos are great for building intuition on the relation between the hypothesis function and the associated cost of that hypothesis for the data.*\n",
    "* [Cost function 1](https://www.youtube.com/watch?v=EANr4YttXIQ&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=6)\n",
    "* [Cost function 2](https://www.youtube.com/watch?v=J5vJFwQWOaY&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=7)\n",
    "\n",
    "Write a function to compute the cost based on the dataset $X$, $R$ and parameters $w_0$ and $w_1$. Based on your plot of the data, try to estimate some sensible values for $w_0$ and $w_1$ and compute the corresponding cost. Try at least 3 different guesses and print their cost. Order the prints of your guesses from highest to lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810641.35213\n",
      "91776.4816584\n",
      "53286.4026687\n"
     ]
    }
   ],
   "source": [
    "def linear_cost(w0, w1, X, R):\n",
    "    N = len(X)\n",
    "    return (1/N)*sum(pow(R-linear_model(w0, w1, X),2))\n",
    "\n",
    "print(linear_cost(100, 500, X, R))\n",
    "print(linear_cost(15, -32, X, R))\n",
    "print(linear_cost(1, 2, X, R))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the linear model [4 pt]\n",
    "\n",
    "We can find the minimum value of the cost function by taking the partial derivatives of that cost function for both of the weights $w_0$ and $w_1$ and setting them equal to $0$, resulting in the equations\n",
    "\n",
    "(2.17a) $$w_1 = \\frac{\\sum_tx^tr^t - \\bar{x}\\bar{r}N}{\\sum_t(x^t)^2 - N\\bar{x}^2}$$\n",
    "(2.17b) $$w_0 = \\bar{r} - w_1\\bar{x}$$\n",
    "\n",
    "You can compute the partial derivates of equation *2.16* yourself and set them both equal to zero, to check you understand where these two equations come from. Minimizing the cost function gives us the best possible parameters for a linear model predicting the values of the provided dataset. *Note:* If you are unfamiliar with the notation $\\bar{x}$, it is defined in *Alpaydin* too, below equation *2.17*.\n",
    "\n",
    "Write a function which computes the optimal values of $w_0$ and $w_1$ for a dataset consisting of the vectors $X$ and $R$, containing $N$ elements each. Use *matplotlib* again to plot the points, but now also add the line representing the hypothesis function you found. As the line is linear, you can simply plot it by computing the 2 end points and have *matplotlib* draw the connecting line.\n",
    "\n",
    "Note that with some clever [array operations](https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html) and [linear algebra](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html) you can avoid explicitly looping over all the elements in $X$ and $R$ in `linear_fit`, which will make you code a lot faster. However, this is just an optional extra and any working implementation of the equations above will be considered correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5d3a0d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear_fit(X, R, N):\n",
    "    X_mean = sum(X) / N\n",
    "    R_mean = sum(R) / N\n",
    "\n",
    "    w1 = (sum(X*R) - X_mean*R_mean*N) / (sum(X**2) -(N*(X_mean**2)))\n",
    "    w0 = R_mean - w1*X_mean\n",
    "    \n",
    "    plt.plot(X, R, 'go')\n",
    "    x1 = linear_model(w0, w1, X[0])\n",
    "    x2 = linear_model(w0, w1, X[-1])\n",
    "    plt.plot([-5,5],[x1, x2], '-b')\n",
    "    plt.scatter(X,R)\n",
    "    plt.show()\n",
    "    \n",
    "linear_fit(X,R,len(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Polynomial data [3 pt]\n",
    "\n",
    "The linear model can easily be extended to polynomials of any order by expanding the original input with the squared input $x^2$, the cubed input $x^3$, etc and adding additional weights to the model. For ease of calculation, the input is also expanded with a vector of $1$'s, to represent the input for the constant parameter $w_0$. The parameters then become $w_0$, $w_1$, $w_2$, etc., one factor for each term of the polynomial.\n",
    "\n",
    "So if originally the dataset of $N$ elements is of the form $X$ (superscripts are indices here)\n",
    "\n",
    "$$ X = \\left[\\begin{array}{c} x^1 \\\\ x^2 \\\\ \\vdots \\\\ x^N \\end{array} \\right]$$\n",
    "\n",
    "Then the matrix $D$ for a $k^{th}$-order polynomial becomes\n",
    "\n",
    "$$ D = \\left[\\begin{array}{cccc}\n",
    "1 & x^1 & (x^1)^2 & \\cdots & (x^1)^k \\\\ \n",
    "1 & x^2 & (x^2)^2 & \\cdots & (x^2)^k \\\\ \n",
    "\\vdots \\\\\n",
    "1 & x^N & (x^N)^2 & \\cdots & (x^N)^k \\\\ \n",
    "\\end{array} \\right]$$\n",
    "\n",
    "Write a function `create_D_matrix` that constructs this matrix for a given vector $X$ up the specified order $k$. Looking at plots for the dataset we have been using so far, the relationship between the points will probably be at least be quadratic. Use the function to construct a matrix $D$ of order $2$, print the matrix and verify that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, -5.0, 25.0],\n",
       " [1.0, -4.6551724137899999, 21.670630202111415],\n",
       " [1.0, -4.3103448275899998, 18.579072532731864],\n",
       " [1.0, -3.9655172413800002, 15.725326991682046],\n",
       " [1.0, -3.6206896551700001, 13.109393579055054],\n",
       " [1.0, -3.27586206897, 10.731272294916408],\n",
       " [1.0, -2.9310344827599999, 8.5909631391281795],\n",
       " [1.0, -2.5862068965499998, 6.6884661117627813],\n",
       " [1.0, -2.2413793103400002, 5.0237812128202144],\n",
       " [1.0, -1.8965517241400001, 3.5969084423384068],\n",
       " [1.0, -1.55172413793, 2.4078478002346015],\n",
       " [1.0, -1.2068965517200001, 1.4565992865536268],\n",
       " [1.0, -0.86206896551699996, 0.74316290130755047],\n",
       " [1.0, -0.51724137931000003, 0.26753864447051134],\n",
       " [1.0, -0.17241379310300001, 0.029726516052164094],\n",
       " [1.0, 0.17241379310300001, 0.029726516052164094],\n",
       " [1.0, 0.51724137931000003, 0.26753864447051134],\n",
       " [1.0, 0.86206896551699996, 0.74316290130755047],\n",
       " [1.0, 1.2068965517200001, 1.4565992865536268],\n",
       " [1.0, 1.55172413793, 2.4078478002346015],\n",
       " [1.0, 1.8965517241400001, 3.5969084423384068],\n",
       " [1.0, 2.2413793103400002, 5.0237812128202144],\n",
       " [1.0, 2.5862068965499998, 6.6884661117627813],\n",
       " [1.0, 2.9310344827599999, 8.5909631391281795],\n",
       " [1.0, 3.27586206897, 10.731272294916408],\n",
       " [1.0, 3.6206896551700001, 13.109393579055054],\n",
       " [1.0, 3.9655172413800002, 15.725326991682046],\n",
       " [1.0, 4.3103448275899998, 18.579072532731864],\n",
       " [1.0, 4.6551724137899999, 21.670630202111415],\n",
       " [1.0, 5.0, 25.0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_D_matrix(X, k):\n",
    "    k_list = [i for i in range(0,k)]\n",
    "    return [[x**j for j in k_list] for x in X]\n",
    "\n",
    "def create_D_matrix(X, k):\n",
    "    matrix = []\n",
    "    for i in X:\n",
    "        temp = []\n",
    "        for j in range(0, k):\n",
    "            temp.append(i**j)\n",
    "        matrix.append(temp)\n",
    "    return matrix\n",
    "\n",
    "D = create_D_matrix(X, 3)\n",
    "create_D_matrix(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial model [2 pt]\n",
    "\n",
    "The parameters can now be represented as\n",
    "\n",
    "$$ w = \\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_k \\end{array} \\right]$$\n",
    "\n",
    "The hypothesis for a single input then just becomes\n",
    "\n",
    "$$ g(x^1) = \\sum_{i=0}^k D^1_iw_i $$\n",
    "\n",
    "Which can write as a matrix multiplication for all inputs in a single equation\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "1 & x^1 & (x^1)^2 & \\cdots & (x^1)^k \\\\ \n",
    "1 & x^2 & (x^2)^2 & \\cdots & (x^2)^k \\\\ \n",
    "\\vdots \\\\\n",
    "1 & x^N & (x^N)^2 & \\cdots & (x^N)^k \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_k \\end{array} \\right] = \\left[\\begin{array}{c} g(x^1) \\\\ g(x^2) \\\\ \\vdots \\\\ g(x^N) \\end{array} \\right]$$\n",
    "\n",
    "You can do matrix multiplication using the [dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) function. Write 2 functions for computing the polynomial below\n",
    "\n",
    "* `poly_val` should take a single input value $x$ and a vector of polynomial weights $W$ and compute the single hypothesis value for that input.\n",
    "* `poly_model` should take a matrix $D$ and weight vector $W$ and compute the corresponding vector of hypotheses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_val(x, W):\n",
    "    return np.dot(x, W)\n",
    "\n",
    "def poly_model(D, W):\n",
    "    return np.dot(D, W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial cost function and model fitting [3 pts]\n",
    "\n",
    "And for the cost function we can now use\n",
    "\n",
    "$$ E(w|X) = \\frac{1}{2N} \\sum_{t=1}^N [r^t - D^tw]^2$$\n",
    "\n",
    "Here, we compute the hypothesis $g(x)$ for every example using $D^tw$, take the difference with the actual output $r$ and finally square and sum each difference. Note that this is extremely similar to the mean squared error function we used for the linear case, and also that minimizing this error function is actually equivalent to maximizing the log likelihood of the parameter vector $w$ (see equations $4.31$ and $4.32$).\n",
    "\n",
    "Now we have the cost function equation and can again take the partial derivative for each of the weights $w_0$ to $w_k$ and set their value equal to $0$. Solving the resulting system of equations will give the set of weights that minimize the cost function. The weights describing this lowest point of the cost function are the parameters which will produce the line that best fits our dataset.\n",
    "\n",
    "Solving all partial derivate equations for each weight can actually be done with just a couple of matrix operations. Deriving the equation yourself can be a bit involved, but know that the principle is exactly the same as for the linear model computing just $w_0$ and $w_1$. The final equation for weight vector becomes\n",
    "\n",
    "(4.33) $$ w = (D^TD)^{-1}D^Tr $$\n",
    "\n",
    "Numpy has built in functions for [transpose](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html) and [inverse](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html). Use them to write the code for the following functions.\n",
    "\n",
    "* `poly_cost` should return the total cost $E$ given $w$, $D$ and $r$\n",
    "* `poly_fit` should return the vector $w$ that bests fits the polynomial relationship between matrix $D$ and vector $r$\n",
    "\n",
    "Using the quadratic matrix $D$ you constructed earlier and this `poly_fit` function, find the best fitting weights for a quadric polynomial on the data and print these weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-56.87348684  52.14107002  16.02439317]\n"
     ]
    }
   ],
   "source": [
    "def poly_cost(W, D, R):\n",
    "    N = len(D)\n",
    "    print(N)\n",
    "    return (1/2*N) * (sum(R - np.dot(D, W)))**2\n",
    "\n",
    "def poly_fit(D, R):\n",
    "    return np.dot(np.linalg.inv(np.dot(np.transpose(D), D)), np.dot(np.transpose(D), R))\n",
    "    \n",
    "best_W = poly_fit(D, R)\n",
    "print(best_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting polynomials [1 pt]\n",
    "\n",
    "Now lets try and figure out what our fitted quadratic polynomial looks like. As the function is not linear, we will need more than just 2 points to actually plot the line. The easiest solution is to create a whole bunch of x-values as samples, compute the corresponding y-values and plot those. With enough samples the line will look smooth, even if it is connected with linear segments.\n",
    "\n",
    "To create these x-values samples, we can use the function [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html). Then just use the `poly_val` function you wrote earlier and apply it to every x-value to compute the array of y-values. Now just plot the original datapoints as dots and the hypothesis as a line, just as for the linear plot. Don't forget to show your plot at the end.\n",
    "\n",
    "Use these steps to fill in the `poly_plot` function below and show the polynomial function defined by the weights you found for the quadratic polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_plot(W, X, R):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial order [1 pt]\n",
    "\n",
    "You can now create a polynomial fit on the data for a polynomial of any order. The next question then becomes: *What order polynomial fits the data the best?*\n",
    "\n",
    "Using the `create_D_matrix`, `poly_fit` and `poly_plot`, try to fit different order polynomials to the data. Show the plot for the order polynomial you think fits best.\n",
    "\n",
    "Note that the cost function will most likely decrease with each added polynomial term, as there is more flexibility in the model to fit the data points exactly. However, these weights will fit those few data points very well, but might have very extreme values in between points that would not be good predictors for new inputs. Something like an order 20 polynomial might have a very well fitting shape for the existing data points, but looks like it would be strange predictor at some of the possible other points. Try to find a fit that looks visually like it would generalize well to new points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation [2 pt]\n",
    "\n",
    "Another way to answer this same question is to use cross validation. With cross validation you split the data into 2 parts and use one part to fit the model (training set) and the other part to see how well the model fits the remaining data (validation set).\n",
    "\n",
    "Write a function below to split the original dataset into 2 sets according to a given ratio. It is important to randomize your division, as simply using the first half of data for the one set and the second half for the other, might result in a strange distribution. You could use a function like [shuffle](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html) for this purpose.\n",
    "\n",
    "Split the original dataset using a ratio of 0.6 into a training and a validation set. Then for both of these sets, use your old `split_X_R` function to split them into their $X$ and $R$ parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(data, ratio):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection [5 pt]\n",
    "\n",
    "With this new split of the data you can just repeatedly fit different order polynomials to the training set and see which produces the lowest cost on the validation set. The set of weights with the lowests cost on the validation set generalizes the best to new data and is thus the best overal fit on the dataset.\n",
    "\n",
    "Write the function `best_poly_fit` below. Try a large range of polynomial orders (like 1 to 50), create the $D$ matrix based on the training set for each order and fit the weights for that polynomial. Then for each of these found weights, also create the D matrix for the validation set and compute the cost using `poly_cost`. Return the set of weights with the lowest cost on the validation set.\n",
    "\n",
    "Run this fitting function with your training and validation sets. Plot the hypothesis function and show the weights that were found. Note that rerunning your validation split code above will result in a different random distribution and thus a slightly different final fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_poly_fit(train_x, train_r, val_x, val_r):\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
